import os

import torch
from torch.utils.data import random_split, DataLoader
from torchvision import datasets
from torchvision.transforms import transforms
from torch.utils.tensorboard import SummaryWriter


writer = SummaryWriter()


def get_loaders(data_dir, batch_size, split=0.9):
    transform = transforms.Compose([
    #transforms.Lambda(lambda image: image.convert('RGB')),
    transforms.Resize(64),
    transforms.CenterCrop(64),
    transforms.ToTensor(),
   # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ])

    dataset = datasets.ImageFolder(root=data_dir, transform=transform)
    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    malware_train, malware_val = random_split(dataset, [train_size, test_size])
    train_dataset = malware_train
    val_dataset = malware_val

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
    drop_last=True)

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        num_workers=0)

    return train_loader, val_loader


def writeMetrics(value_dict, step):
    # for key, value in value_dict.items():
    #     writer.add_scalar(key, value, step)
    writer.add_scalars("D/G Loss", value_dict, step)



def load_checkpoint(generator, discriminator, g_optimizer, d_optimizer, save_path):
    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.
    ngpu = 1
    device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")
    start_epoch = 0
    #filename = save_path + "DCGAN" + ".pth"
    filename = save_path
    if os.path.isfile(filename):
        print("=> loading checkpoint '{}'".format(filename))
        checkpoint = torch.load(filename)
        start_epoch = checkpoint['epoch']
        generator.load_state_dict(checkpoint['generator_state_dict'])
        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
        g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])
        d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])
        batch_num = checkpoint['batch_num']
        print("=> loaded checkpoint '{}' (epoch {})"
              .format(filename, checkpoint['epoch']))

        generator = generator.to(device)
        discriminator = discriminator.to(device)
        # now individually transfer the optimizer parts...
        for state in g_optimizer.state.values():
            for k, v in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.to(device)

        for state in d_optimizer.state.values():
            for k, v in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.to(device)

    
    else:
        print("=> no checkpoint found at '{}'".format(filename))
        quit()

    

    return generator, discriminator, g_optimizer, d_optimizer, start_epoch, batch_num




def checkpoint(generator, discriminator, g_optimizer, d_optimizer, epoch, batch_num, save_path, name):
    print(f"Saving ======> Epoch is {epoch}")
    torch.save({
        'generator_state_dict': generator.state_dict(),
        'discriminator_state_dict': discriminator.state_dict(),
        'epoch': epoch,
        'g_optimizer_state_dict': g_optimizer.state_dict(),
        'd_optimizer_state_dict': d_optimizer.state_dict(),
        'batch_num': batch_num
    }, f"{save_path}{name}_{str(epoch)}.pth")
