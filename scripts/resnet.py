import pytorch_lightning as pl
import torch
import torchvision.datasets as dset
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.nn import functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import torch.nn as nn

seed_everything(0)


class Resnet18(pl.LightningModule):
    def __init__(self, model, num_workers):
        super().__init__()
        self.save_hyperparameters()
        self.model = model
        self.training_correct_counter = 0
        self.num_workers = num_workers
        # self.train_loader = train_loader
        # self.val_loader = val_loader

    def forward(self, x):
        return self.model(x)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=batchsize, pin_memory=True,
                          shuffle=True, num_workers=self.num_workers)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'train_loss': loss}

        return {'loss': loss, 'log': tensorboard_logs}

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=batchsize, pin_memory=True,
                          shuffle=False, num_workers=self.num_workers)

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)

        return {'val_loss': loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        return {'val_loss': avg_loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

    def prepare_data(self):
        transform =T.Compose([
        T.Resize(256),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        dataset = dset.ImageFolder(root=DATA_DIR, transform=transform)
        train_size = int(0.8 * len(dataset))
        test_size = len(dataset) - train_size
        malware_train, malware_val = random_split(dataset, [train_size, test_size])
        self.train_dataset = malware_train
        self.val_dataset = malware_val


# define variables
epoch = 1
n_class = 25
debug = False
batchsize = 64
num_workers = 4

DATA_DIR = "/run/media/alex/New Volume/malware_binary/MC-dataset-multiclass/image_multiclass"
root_path = '.'
checkpoint_callback = ModelCheckpoint(
    filepath=root_path,
    save_top_k=1,
    verbose=True,
    monitor='val_loss',
    mode='min'
)
model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)
for name, param in model.named_parameters():
    if ("bn" not in name):
        param.requires_grad = False
num_classes = 5

model.fc = nn.Sequential(nn.Linear(model.fc.in_features,512),
                                  nn.ReLU(),
                                  nn.Dropout(),
                                  nn.Linear(512, num_classes))

model.train()

# model
pl_model = Resnet18(model, 4)

# most basic trainer, uses good defaults
trainer = Trainer(default_root_dir=root_path, gpus=0, max_epochs=100,
                  checkpoint_callback=checkpoint_callback, early_stop_callback=True)
trainer.fit(pl_model)
trainer.save_checkpoint("final_epoch_resnet18.ckpt")
