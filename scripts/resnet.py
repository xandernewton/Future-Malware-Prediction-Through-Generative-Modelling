import argparse
from collections import OrderedDict

import pytorch_lightning as pl
import torch
import torch.nn as nn
import torchvision.datasets as dset
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.callbacks import EarlyStopping
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.nn import functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T

seed_everything(0)


class Resnet18(pl.LightningModule):

    def __init__(self, model, num_workers, batch_size):
        super().__init__()
        self.save_hyperparameters()
        self.model = model
        self.training_correct_counter = 0
        self.num_workers = num_workers
        self.batchsize = batch_size


    def forward(self, x):
        return self.model(x)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batchsize, pin_memory=True,
                          shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batchsize, pin_memory=True,
                          shuffle=False)

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)

        return {'val_loss': loss}

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'train_loss': loss}

        return {'loss': loss, 'log': tensorboard_logs}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        return {'val_loss': avg_loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

    def prepare_data(self):

        transform = T.Compose([
            T.Resize(256),
            T.CenterCrop(224),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        dataset = dset.ImageFolder(root=DATA_DIR, transform=transform)
        train_size = int(0.8 * len(dataset))
        test_size = len(dataset) - train_size
        malware_train, malware_val = random_split(dataset, [train_size, test_size])
        self.train_dataset = malware_train
        self.val_dataset = malware_val
        if args.checkpoint_path:
            self.train_dataset = dset.ImageFolder(root=args.extra_images_path, transform=transform)



def getModel():
    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)
    for name, param in model.named_parameters():
        if ("bn" not in name):
            param.requires_grad = False
    num_classes = 5

    model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 512),
                             nn.ReLU(),
                             nn.Dropout(),
                             nn.Linear(512, num_classes))
    model.train()
    return model


def load_pytorch_model(state_dict):


    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k
        if name.startswith('model.'):
            name = name.replace('model.', '')  # remove `model.`
        new_state_dict[name] = v

    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)
    model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 512),
                             nn.ReLU(),
                             nn.Dropout(),
                             nn.Linear(512, 5))

    model.load_state_dict(new_state_dict)
    return model

def continue_training(checkpoint_path ,name):
    device = torch.device("cuda:0" if (torch.cuda.is_available()) else "cpu")

    ckpt_dict = torch.load(checkpoint_path, map_location=device)
    model = load_pytorch_model(ckpt_dict['state_dict'])
    pl_model = Resnet18(model, 4, batch_size=batchsize)
    checkpoint_callback = ModelCheckpoint(
        filepath=root_path,
        save_top_k=1,
        verbose=True,
        monitor='val_loss',
        mode='min'
    )
    early_stop_callback = EarlyStopping(
        monitor='val_loss',
        min_delta=0.00,
        patience=5,
        verbose=False,
        mode='auto'
    )

    # most basic trainer, uses good defaults
    trainer = Trainer(default_root_dir=root_path, gpus=0, max_epochs=100,
                      checkpoint_callback=checkpoint_callback, early_stop_callback=early_stop_callback)
    trainer.fit(pl_model)
    trainer.save_checkpoint(f"resnet18_{name}.ckpt")





if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='Generic runner for VAE models')
    parser.add_argument('--checkpoint_path', '-c', type=str,
                        help='Trains on extra images')
    parser.add_argument('--name', '-n', type=str,
                        help='Trains on extra images')

    parser.add_argument('--extra_images_path', '-e', type=str,
                        help='Path to extra images')

    args = parser.parse_args()

    DATA_DIR = "C:\\Users\\ahrn1e19\\multiclass\\image_multiclass\\"
    DATA_DIR = "/run/media/alex/New Volume/malware_binary/MC-dataset-multiclass/image_multiclass"
    root_path = 'C:\\Users\\ahrn1e19'
    root_path = '.'
    # define variables
    epoch = 1
    n_class = 25
    debug = False
    batchsize = 64
    num_workers = 0

    if args.checkpoint_path:
        continue_training(args.checkpoint_path, args.name)

    else:

        checkpoint_callback = ModelCheckpoint(
            filepath=root_path,
            save_top_k=1,
            verbose=True,
            monitor='val_loss',
            mode='min'
        )

        # model
        model = getModel()
        pl_model = Resnet18(model, 4, batch_size=batchsize)

        early_stop_callback = EarlyStopping(
            monitor='val_loss',
            min_delta=0.00,
            patience=5,
            verbose=False,
            mode='auto'
        )

        # most basic trainer, uses good defaults
        trainer = Trainer(default_root_dir=root_path, gpus=0, max_epochs=100,
                          checkpoint_callback=checkpoint_callback, early_stop_callback=early_stop_callback)
        trainer.fit(pl_model)
        trainer.save_checkpoint("final_epoch_resnet18.ckpt")
