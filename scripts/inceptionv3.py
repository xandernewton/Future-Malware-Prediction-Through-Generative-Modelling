import pytorch_lightning as pl
import torch
import torchvision.datasets as dset
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.callbacks import EarlyStopping
from torch.nn import functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T

seed_everything(0)


class InceptionV3(pl.LightningModule):
    def __init__(self, model):
        super().__init__()
        self.save_hyperparameters()
        self.model = model
        self.training_correct_counter = 0
        # self.train_loader = train_loader
        # self.val_loader = val_loader

    def forward(self, x):
        return self.model(x)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=batchsize, pin_memory=True,
                          shuffle=True)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'train_loss': loss}

        return {'loss': loss, 'log': tensorboard_logs}

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=batchsize, pin_memory=True,
                          shuffle=False)

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)

        return {'val_loss': loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        return {'val_loss': avg_loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

    def prepare_data(self):
        transform = T.Compose([
            # transforms.Lambda(lambda image: image.convert('RGB')),
            T.CenterCrop(229),
            T.Resize(229),
            T.ToTensor()
        ])

        dataset = dset.ImageFolder(root=DATA_DIR, transform=transform)
        train_size = int(0.8 * len(dataset))
        test_size = len(dataset) - train_size
        malware_train, malware_val = random_split(dataset, [train_size, test_size])
        self.train_dataset = malware_train
        self.val_dataset = malware_val


# define variables
epoch = 1
n_class = 25
debug = False
batchsize = 64
num_workers = 4

DATA_DIR = "C:\\Users\\ahrn1e19\\multiclass\\image_multiclass"
root_path = 'H:\\MSc-project'
checkpoint_callback = ModelCheckpoint(
    filepath=root_path,
    save_top_k=1,
    verbose=True,
    monitor='val_loss',
    mode='min'
)

early_stop_callback = EarlyStopping(
   monitor='val_loss',
   min_delta=0.00,
   patience=10,
   verbose=False,
   mode='auto'
)



model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', num_classes=5, aux_logits=False, transform_input=False, pretrained=False)
model.train()

# model
pl_model = InceptionV3(model)

# most basic trainer, uses good defaults
trainer = Trainer(default_root_dir=root_path, gpus=1, max_epochs=35,
                  checkpoint_callback=checkpoint_callback, early_stop_callback=early_stop_callback)
trainer.fit(pl_model)
trainer.save_checkpoint("final_epoch.ckpt")
