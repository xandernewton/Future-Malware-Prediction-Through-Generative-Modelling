# -*- coding: utf-8 -*-
"""inceptionv3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15M6DARiXh_meu1f9WRrpmL4NiWCTmG0e
"""

# define variables
epoch = 1
n_class = 25
DATA_DIR = "/content/drive/My Drive/southampton/project/malware_binary/malimg/malimg_paper_dataset_imgs"
debug = False
batchsize = 64
num_workers = 4

import os

import torch
from torch.nn import functional as F
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from torch.utils.data import Dataset
import torchvision.datasets as dset
from torchvision import transforms as T

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping


class InceptionV3(pl.LightningModule):
  def __init__(self, model, train_loader, val_loader):
    super().__init__()
    self.save_hyperparameters()
    self.model = model
    self.training_correct_counter = 0
    self.train_loader = train_loader
    self.val_loader = val_loader

  def forward(self, x):
      return self.model(x)

  def training_step(self, batch, batch_idx):
      x, y = batch
      y_hat = self(x)      
      loss = F.cross_entropy(y_hat, y)
      tensorboard_logs = {'train_loss': loss}

      # if batch_idx == 0:
      #   self.training_correct_counter = (torch.max(y_hat, 1)[1].view(y.size()) == y).sum()
      # else:
      #   self.training_correct_counter += (torch.max(y_hat, 1)[1].view(y.size()) == y).sum()

      return {'loss': loss, 'log': tensorboard_logs}

  def validation_step(self, batch, batch_idx):
      x, y = batch
      y_hat = self(x)
      loss = F.cross_entropy(y_hat, y)

      return {'val_loss': loss}

  def validation_epoch_end(self, outputs):
      avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
      #train_avg_acc = 100 * self.training_correct_counter / len(self.train_loader.dataset)
      return {'val_loss': avg_loss}#, 'val_accuracy':train_avg_acc}

  def configure_optimizers(self):
      return torch.optim.Adam(self.parameters(), lr=0.001)

transform = T.Compose([
                # transforms.Lambda(lambda image: image.convert('RGB')),
                T.CenterCrop(229),
                T.Resize(229),
                T.ToTensor()
            ])

dataset = dset.ImageFolder(root=DATA_DIR, transform=transform)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
malware_train, malware_val = random_split(dataset, [train_size, test_size])

train_dataset = malware_train
val_dataset = malware_val

train_loader = DataLoader(train_dataset, batch_size=batchsize, pin_memory=True,
                          shuffle=True, num_workers = num_workers)
val_loader = DataLoader(val_dataset, batch_size=batchsize, pin_memory=True,
                          shuffle=False, num_workers = num_workers)

checkpoint_callback = ModelCheckpoint(filepath='.')
early_stop_callback = EarlyStopping(
   monitor='val_accuracy',
   min_delta=0.00,
   patience=3,
   verbose=False,
   mode='max'
)

from pytorch_lightning import Trainer, seed_everything
seed_everything(0)

model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', num_classes=25, aux_logits=False, transform_input=False,
                 inception_blocks=None, init_weights=None)
model.train()

# model
pl_model = InceptionV3(model, train_loader, val_loader)

# most basic trainer, uses good defaults
trainer = Trainer(num_tpu_cores=8, max_epochs=20, checkpoint_callback=checkpoint_callback)#, early_stop_callback=early_stop_callback)
trainer.fit(pl_model, train_loader, val_loader)



quit()
from collections import OrderedDict
from tqdm import tqdm
def load_pytorch_model(state_dict, *args, **kwargs):
    new_state_dict = OrderedDict()
    for k, v in tqdm(state_dict.items(), total=len(state_dict.items())):
        name = k
        if name.startswith('model.'):
            name = name.replace('model.', '') # remove `model.`
        new_state_dict[name] = v
    model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', num_classes=25, aux_logits=False, transform_input=False,
                 inception_blocks=None, init_weights=None)
    model.load_state_dict(new_state_dict)
    return model

# load best model
#from pathlib import Path
#ckpt_path = list(Path('.').glob('*.ckpt'))[0]
ckpt_path = "/content/drive/My Drive/southampton/epoch=15.ckpt"
print("pass")
ckpt_dict = torch.load(ckpt_path)
print("pass 2")
torch.save(ckpt_dict['state_dict'],"epoch_15.pth")
best_model = load_pytorch_model(ckpt_dict['state_dict'])

def predict(model, dataloader, n_class, device, tta=1):
    model.eval()
    model.to(device)
    preds = np.zeros([0, n_class])
    labels = []
    for data, label in dataloader:
        data = data.to(device)
        with torch.no_grad():
            y_pred = model(data).detach()
        #y_pred = F.softmax(y_pred, dim=1).cpu().numpy()
        y_pred = y_pred.cpu().numpy()
        preds = np.concatenate([preds, y_pred])
        labels = [*labels, *(label.tolist()) ]
    return preds, labels

from sklearn.metrics import accuracy_score
import numpy as np

ngpu = 1
device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")
val_preds, labels = predict(best_model, val_loader, n_class=25, device=device)
print(f"Labels are {labels}")
val_acc = accuracy_score(labels, np.argmax(val_preds, axis=1))
print(f'val acc: {val_acc}')




