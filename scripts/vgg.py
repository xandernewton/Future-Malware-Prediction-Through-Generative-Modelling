import argparse
from collections import OrderedDict

import pytorch_lightning as pl
import torch
import torch.nn as nn
import torchvision.datasets as dset
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.callbacks import EarlyStopping
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.nn import functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
from torchvision import models

seed_everything(0)


class VGG(pl.LightningModule):

    def __init__(self, model, num_workers, batch_size):
        super().__init__()
        self.save_hyperparameters()
        self.model = model
        self.training_correct_counter = 0
        self.num_workers = num_workers
        self.batchsize = batch_size


    def forward(self, x):
        return self.model(x)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batchsize, pin_memory=True,
                          shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batchsize, pin_memory=True,
                          shuffle=False)

    def validation_step(self, batch, batch_idx):
        self.model.eval()
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)

        return {'val_loss': loss}

    def training_step(self, batch, batch_idx):
        self.model.train()
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'train_loss': loss}

        return {'loss': loss, 'log': tensorboard_logs}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        return {'val_loss': avg_loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

    def prepare_data(self):

        transform = T.Compose([
            T.Resize(256),
            T.CenterCrop(224),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        dataset = dset.ImageFolder(root=DATA_DIR, transform=transform)
        train_size = int(0.8 * len(dataset))
        test_size = len(dataset) - train_size
        malware_train, malware_val = random_split(dataset, [train_size, test_size])
        self.train_dataset = malware_train
        self.val_dataset = malware_val
        # dataset = dset.ImageFolder(root=DATA_DIR, transform=transform)
        # self.train_dataset = dataset



def getModel():

    model = models.vgg16(pretrained=True)
    num_features = model.classifier[6].in_features
    features = list(model.classifier.children())[:-1]  # Remove last layer
    features.extend([nn.Linear(num_features, 5)])  # Add our layer with 4 outputs
    model.classifier = nn.Sequential(*features)  # Replace the model classifier
    model.train()
    return model


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='Generic runner for VAE models')
    parser.add_argument('--checkpoint_path', '-c', type=str,
                        help='Trains on extra images')
    parser.add_argument('--name', '-n', type=str,
                        help='Trains on extra images')

    parser.add_argument('--extra_images_path', '-e', type=str,
                        help='Path to extra images')

    args = parser.parse_args()

    #DATA_DIR = "C:\\Users\\ahrn1e19\\binary_classification_2"
    #VAL_DIR = "C:\\Users\\ahrn1e19\\val_dataset"
    DATA_DIR = "/run/media/alex/New Volume/malware_binary/MC-dataset-multiclass/image_multiclass"
    #root_path = 'C:\\Users\\ahrn1e19'
    root_path = '.'
    # define variables
    epoch = 1
    n_class = 25
    debug = False
    batchsize = 64
    num_workers = 0


    checkpoint_callback = ModelCheckpoint(
        filepath=root_path,
        save_top_k=1,
        verbose=True,
        monitor='val_loss',
        mode='min'
    )

    # model
    model = getModel()
    pl_model = VGG(model, 4, batch_size=batchsize)

    early_stop_callback = EarlyStopping(
        monitor='val_loss',
        min_delta=0.00,
        patience=3,
        verbose=False,
        mode='auto'
    )

    # most basic trainer, uses good defaults
    trainer = Trainer(default_root_dir=root_path, gpus=0, max_epochs=100,
                      checkpoint_callback=checkpoint_callback, early_stop_callback=early_stop_callback)
    trainer.fit(pl_model)
    #trainer.save_checkpoint(f"final_epoch_resnet18_{args.name}.ckpt")
