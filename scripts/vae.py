# -*- coding: utf-8 -*-
"""VAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VLvXEAAuYYrXcRCNfC7yiXrTq9DCXIjD
"""



# Root directory for dataset
from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn


class EncoderBlock(nn.Module):
    def __init__(self, base_channel):
        super().__init__()
        self.base_channel = base_channel
        self.conv = nn.Sequential(
            nn.Conv2d(
                in_channels=3,
                out_channels=self.base_channel,
                kernel_size=4, padding=1, stride=2),  # 16
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(
                in_channels=self.base_channel,
                out_channels=self.base_channel*2,
                kernel_size=4, padding=1, stride=2),  # 8
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(
                in_channels=self.base_channel*2,
                out_channels=self.base_channel*4,
                kernel_size=4, padding=1, stride=2),  # 4
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(
                in_channels=self.base_channel*4,
                out_channels=self.base_channel*8,
                kernel_size=4, padding=1, stride=2),  # 2
            nn.LeakyReLU(0.2, inplace=True),
        )

    def forward(self, x):
        return self.conv(x)


class UpsampleDecoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.latent_dim = latent_dim
        base_channel = 64
        self.network = nn.Sequential(
            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True),
            nn.Conv2d(
                in_channels=latent_dim,
                out_channels=base_channel*8,
                bias=False,
                kernel_size=3, padding=1),
            nn.BatchNorm2d(num_features=base_channel*8),
            nn.ReLU(True),  # 4

            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            nn.Conv2d(in_channels=base_channel*8,
                      out_channels=base_channel*4,
                      bias=False,
                      kernel_size=3, padding=1),
            nn.BatchNorm2d(num_features=base_channel*4),
            nn.ReLU(True),  # 8

            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            nn.Conv2d(in_channels=base_channel*4,
                      out_channels=base_channel*2,
                      bias=False,
                      kernel_size=3, padding=1),
            nn.BatchNorm2d(num_features=base_channel*2),
            nn.ReLU(True),  # 16

            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            nn.Conv2d(in_channels=base_channel*2,
                      out_channels=3,
                      kernel_size=3, padding=1),
            nn.Tanh()  # 32
        )

    def forward(self, x):
        return self.network(x.unsqueeze(-1).unsqueeze(-1))


class VAE(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.latent_dim = latent_dim
        base_channel = 64
        self.lin_in_dim = 2*2*base_channel*8

        # define encoder block
        self.encoder = EncoderBlock(base_channel)

        self.lin1 = nn.Sequential(
            nn.Linear(self.lin_in_dim, latent_dim),
            nn.ReLU(),
        )

        # linear layers for mu and logvar prediction
        self.lin11 = nn.Linear(latent_dim, latent_dim)
        self.lin12 = nn.Linear(latent_dim, latent_dim)

        # decoder block
        self.decoder = UpsampleDecoder(latent_dim)

    def reparametrize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return eps.mul(std).add_(mu)

    def encode(self, x):
        z = self.encoder(x)
        z = z.view(-1, self.lin_in_dim)
        z = self.lin1(z)
        mu = self.lin11(z)
        logvar = self.lin12(z)
        z = self.reparametrize(mu, logvar)
        return z, mu, logvar

    def forward(self, x):
        z, mu, logvar = self.encode(x)
        x_hat = self.decoder(z)
        return x_hat, mu, logvar

from torch.utils.data import DataLoader
from torch.utils.data.dataset import random_split
from torchvision import transforms
from torchvision.datasets import SVHN, ImageFolder
import torchvision.datasets as dset

def get_loaders(data_dir, batch_size, split=0.9):
    transform = transforms.Compose([
        transforms.CenterCrop(32),
        transforms.Resize(32),
        transforms.ToTensor(),
        transforms.Normalize((.5, .5, .5),
                             (.5, .5, .5))
    ])

    data_dir = "/content/drive/My Drive/southampton/project/malware_binary/malimg/malimg_paper_dataset_imgs"

    dataset = dset.ImageFolder(root=data_dir, transform=transform)
    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    malware_train, malware_val = random_split(dataset, [train_size, test_size])
    train_dataset = malware_train
    val_dataset = malware_val
   
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4)

    valid_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        num_workers=4)

    return train_loader, valid_loader

import torch
import torch.nn as nn
from torch.optim import Adam
from tqdm.notebook import tqdm



def compute_loss(inputs, outputs, mu, logvar):
    reconstruction_loss = nn.MSELoss(reduction='sum')(inputs, outputs)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())
    return kl_loss + reconstruction_loss


def train_vae():

    batch_size = 64
    epochs = 100
    latent_dimension = 100
    patience = 10

    device = torch.device('cuda:0') \
        if torch.cuda.is_available() \
        else torch.device('cpu')
    print(f"device is {device}")

    # load data
    train_loader, valid_loader = get_loaders('data', batch_size)

    model = VAE(latent_dimension).to(device)

    optim = Adam(model.parameters(), lr=1e-3)

    # intialize variables for early stopping
    val_greater_count = 0
    last_val_loss = 0

    for e in tqdm(range(epochs)):
        running_loss = 0
        model.train()
        for inx, (images, _) in enumerate(train_loader):
            images = images.to(device)
            model.zero_grad()
            outputs, mu, logvar = model(images)
            loss = compute_loss(images, outputs, mu, logvar)
            running_loss += loss
            loss.backward()
            optim.step()
            print(f"Batch num is {inx} / {len(train_loader)}")
     

        running_loss = running_loss/len(train_loader)
        model.eval()
        with torch.no_grad():
            val_loss = 0
            for images, _ in valid_loader:
                images = images.to(device)
                outputs, mu, logvar = model(images)
                loss = compute_loss(images, outputs, mu, logvar)
                val_loss += loss
            val_loss /= len(valid_loader)
            
        # increment variable for early stopping
        if val_loss > last_val_loss:
            val_greater_count += 1
        else:
            val_greater_count = 0
        last_val_loss = val_loss

        # save model
        torch.save({
            'epoch': e,
            'model': model.state_dict(),
            'running_loss': running_loss,
            'optim': optim.state_dict(),
        }, "checkpoint_{}.pth".format(e))
        print("Epoch: {} Train Loss: {}".format(e+1, running_loss.item()))
        print("Epoch: {} Val Loss: {}".format(e+1, val_loss.item()))

        # check early stopping condition
        if val_greater_count >= patience:
            break


if __name__ == '__main__':
    train_vae()



